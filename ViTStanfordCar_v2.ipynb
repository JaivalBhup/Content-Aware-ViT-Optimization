{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c5f86c9a-fbf1-4bb1-80b2-9d9018f048d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "from PIL import ImageDraw, ImageFont, Image\n",
    "from transformers import ViTImageProcessor, ViTForImageClassification, TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "from evaluate import load\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c221ace0-d3e6-4155-9517-6770f87e2131",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image_file_path': 'StanfordCars/cars_train/cars_train/04354.jpg', 'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=720x480 at 0x347717740>, 'label_name': 'Jeep Wrangler SUV 2012', 'labels': 0}\n"
     ]
    }
   ],
   "source": [
    "file_path = \"StanfordCars/stanford_cars_with_class_names.xlsx\"\n",
    "train_df = pd.read_excel(file_path, sheet_name='train')\n",
    "\n",
    "train_folder = \"StanfordCars/cars_train/cars_train\"\n",
    "\n",
    "# Get image files\n",
    "train_image_files = [f for f in os.listdir(train_folder) if f.lower().endswith(('png', 'jpg', 'jpeg', 'bmp', 'tiff'))]\n",
    "\n",
    "# Prepare dataset components\n",
    "image_paths = []\n",
    "images = []\n",
    "labels1 = []\n",
    "label_ids1 = []\n",
    "count = 0\n",
    "label_to_id = {}  # To dynamically assign new label IDs\n",
    "id_to_label = {}\n",
    "next_label_id = 0\n",
    "\n",
    "for image_file in train_image_files:\n",
    "    image_path = os.path.join(train_folder, image_file)\n",
    "    count += 1\n",
    "    if count > 200:\n",
    "        break\n",
    "    try:\n",
    "        with Image.open(image_path) as img:\n",
    "            metadata = train_df.loc[train_df['image'] == image_file]\n",
    "            if not metadata.empty:\n",
    "                label_name = metadata['ture_class_name'].values[0]\n",
    "                \n",
    "                # Dynamically assign a new label ID if label_name is new\n",
    "                if label_name not in label_to_id:\n",
    "                    label_to_id[label_name] = next_label_id\n",
    "                    id_to_label[next_label_id] = label_name\n",
    "                    next_label_id += 1\n",
    "                \n",
    "                \n",
    "                image_paths.append(image_path)\n",
    "                images.append(img.copy())  # Use .copy() to avoid closing the image later\n",
    "                #labels1.append(metadata['ture_class_name'].values[0])\n",
    "                #label_ids1.append(metadata['class'].values[0])\n",
    "                labels1.append(label_name)\n",
    "                label_ids1.append(label_to_id[label_name])\n",
    "    except ValueError as ve:\n",
    "        print(f\"ValueError encountered with image: {image_file}, skipping it.\\n\")\n",
    "\n",
    "# Create a dictionary to match the Hugging Face dataset format\n",
    "data_dict = {\n",
    "    \"image_file_path\": image_paths,\n",
    "    \"image\": images,\n",
    "    \"label_name\": labels1,\n",
    "    \"labels\": label_ids1,\n",
    "}\n",
    "\n",
    "# Load into a Hugging Face Dataset\n",
    "train_dataset = Dataset.from_dict(data_dict)\n",
    "\n",
    "# Save the dataset locally or explore it\n",
    "#train_dataset.save_to_disk(\"stanford_cars_train_dataset\")\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "8b8579a2-7192-45a3-9e72-1948fd0d95f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'image_file_path': 'StanfordCars/cars_train/cars_train/04354.jpg', 'image': <PIL.PngImagePlugin.PngImageFile image mode=RGB size=720x480 at 0x3478476B0>, 'label_name': 'Jeep Wrangler SUV 2012', 'labels': 0}\n"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_excel(file_path, sheet_name='train')\n",
    "\n",
    "test_folder = \"StanfordCars/cars_test/cars_test\"\n",
    "\n",
    "# Get image files\n",
    "test_image_files = [f for f in os.listdir(test_folder) if f.lower().endswith(('png', 'jpg', 'jpeg', 'bmp', 'tiff'))]\n",
    "\n",
    "# Prepare dataset components\n",
    "image_paths = []\n",
    "images = []\n",
    "labels2 = []\n",
    "label_ids2 = []\n",
    "count = 0\n",
    "\n",
    "\n",
    "\n",
    "for image_file in test_image_files:\n",
    "    image_path = os.path.join(train_folder, image_file)\n",
    "    count += 1\n",
    "    if count > 200:\n",
    "        break\n",
    "    try:\n",
    "        with Image.open(image_path) as img:\n",
    "            metadata = train_df.loc[test_df['image'] == image_file]\n",
    "            if not metadata.empty:\n",
    "                label_name = metadata['ture_class_name'].values[0]\n",
    "                \n",
    "                # Dynamically assign a new label ID if label_name is new\n",
    "                if label_name not in label_to_id:\n",
    "                    label_to_id[label_name] = next_label_id\n",
    "                    id_to_label[next_label_id] = label_name\n",
    "                    next_label_id += 1\n",
    "                \n",
    "                image_paths.append(image_path)\n",
    "                images.append(img.copy())  # Use .copy() to avoid closing the image later\n",
    "                labels2.append(label_name)\n",
    "                label_ids2.append(label_to_id[label_name])\n",
    "    except ValueError as ve:\n",
    "        print(f\"ValueError encountered with image: {image_file}, skipping it.\\n\")\n",
    "\n",
    "# Create a dictionary to match the Hugging Face dataset format\n",
    "data_dict = {\n",
    "    \"image_file_path\": image_paths,\n",
    "    \"image\": images,\n",
    "    \"label_name\": labels2,\n",
    "    \"labels\": label_ids2,\n",
    "}\n",
    "\n",
    "# Load into a Hugging Face Dataset\n",
    "test_dataset = Dataset.from_dict(data_dict)\n",
    "\n",
    "# Save the dataset locally or explore it\n",
    "#train_dataset.save_to_disk(\"stanford_cars_train_dataset\")\n",
    "print(test_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "2d65963a-947d-4e11-9662-22982f8cd375",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTImageProcessor {\n",
       "  \"do_normalize\": true,\n",
       "  \"do_rescale\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"image_mean\": [\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5\n",
       "  ],\n",
       "  \"image_processor_type\": \"ViTImageProcessor\",\n",
       "  \"image_std\": [\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5\n",
       "  ],\n",
       "  \"resample\": 2,\n",
       "  \"rescale_factor\": 0.00392156862745098,\n",
       "  \"size\": {\n",
       "    \"height\": 224,\n",
       "    \"width\": 224\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name_or_path = 'google/vit-base-patch16-224-in21k'\n",
    "processor = ViTImageProcessor.from_pretrained(model_name_or_path)\n",
    "#processor.size = {'height': 180, 'width': 180}\n",
    "processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "41a46d88-243f-455f-b1ed-a614503fd630",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pixel_values': tensor([[[ 0.4039,  0.4118,  0.4118,  ...,  1.0000,  1.0000,  1.0000],\n",
       "          [ 0.4118,  0.4039,  0.4118,  ...,  1.0000,  1.0000,  1.0000],\n",
       "          [ 0.4118,  0.4039,  0.4118,  ...,  1.0000,  1.0000,  1.0000],\n",
       "          ...,\n",
       "          [-0.0824, -0.3412, -0.3490,  ...,  0.2627,  0.3647,  0.3098],\n",
       "          [-0.1294, -0.1294, -0.1765,  ...,  0.4588,  0.4510,  0.3647],\n",
       "          [-0.2157, -0.1137,  0.0431,  ...,  0.4667,  0.4980,  0.4824]],\n",
       " \n",
       "         [[ 0.4196,  0.4275,  0.4275,  ...,  1.0000,  1.0000,  1.0000],\n",
       "          [ 0.4275,  0.4196,  0.4275,  ...,  1.0000,  1.0000,  1.0000],\n",
       "          [ 0.4275,  0.4196,  0.4275,  ...,  1.0000,  1.0000,  1.0000],\n",
       "          ...,\n",
       "          [-0.1059, -0.3647, -0.3569,  ...,  0.2471,  0.3098,  0.2471],\n",
       "          [-0.1216, -0.1137, -0.1451,  ...,  0.4196,  0.3569,  0.2863],\n",
       "          [-0.1922, -0.1137,  0.0824,  ...,  0.4118,  0.4196,  0.3882]],\n",
       " \n",
       "         [[ 0.5137,  0.5294,  0.5373,  ...,  0.9843,  0.9843,  0.9843],\n",
       "          [ 0.5216,  0.5216,  0.5373,  ...,  0.9843,  0.9843,  0.9843],\n",
       "          [ 0.5216,  0.5216,  0.5373,  ...,  0.9843,  0.9843,  0.9843],\n",
       "          ...,\n",
       "          [-0.2863, -0.5216, -0.5373,  ...,  0.1843,  0.2235,  0.1373],\n",
       "          [-0.3961, -0.3961, -0.4196,  ...,  0.3255,  0.2549,  0.1608],\n",
       "          [-0.5216, -0.4667, -0.3020,  ...,  0.3255,  0.3176,  0.2549]]]),\n",
       " 'labels': 0}"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transform(example_batch):\n",
    "    #print(example_batch)\n",
    "    # Take a list of PIL images and turn them to pixel values\n",
    "    inputs = processor([x for x in example_batch['image']], return_tensors='pt')\n",
    "\n",
    "    # Don't forget to include the labels!\n",
    "    inputs['labels'] = example_batch['labels']\n",
    "    return inputs\n",
    "\n",
    "prepared_train_ds = train_dataset.with_transform(transform)\n",
    "prepared_test_ds = test_dataset.with_transform(transform)\n",
    "prepared_train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "39d68e4e-4a10-4047-9376-d85894ef7461",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
    "        'labels': torch.tensor([x['labels'] for x in batch])\n",
    "    }\n",
    "\n",
    "metric = load(\"accuracy\")\n",
    "def compute_metrics(p):\n",
    "    return metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids)\n",
    "\n",
    "#all_labels = labels1 + labels2\n",
    "#all_label_ids = label_ids1 + label_ids2\n",
    "#label_to_id = {label: int(label_id) for label, label_id in zip(all_labels, all_label_ids)}\n",
    "#id_to_label = {int(label_id): label for label, label_id in zip(all_labels, all_label_ids)}\n",
    "#print(label_to_id)\n",
    "\n",
    "labels = list(label_to_id.keys())\n",
    "#print(labels)\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    num_labels=len(labels),\n",
    "    id2label=id_to_label,\n",
    "    label2id=label_to_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "964f87c8-dc28-47e0-aa40-3f539f9d0247",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/training_args.py:1583: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of 🤗 Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./vit-model\",\n",
    "    per_device_train_batch_size=16,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    num_train_epochs=4,\n",
    "    fp16=False,\n",
    "    no_cuda=True,\n",
    "    save_steps=100,\n",
    "    eval_steps=100,\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-4,\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    "    report_to='tensorboard',\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "330ef30c-a39c-4e43-93f2-d00e2c49b8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/xx/6trqhb9n4yq6254vft620z0r0000gn/T/ipykernel_7472/4000150063.py:1: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=prepared_train_ds,\n",
    "    eval_dataset=prepared_test_ds,\n",
    "    tokenizer=processor,\n",
    "    data_collator=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "38c37ebc-e0eb-42f5-be2f-0eaa7ea69831",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='52' max='52' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [52/52 01:46, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =        4.0\n",
      "  total_flos               = 57802267GF\n",
      "  train_loss               =     4.5595\n",
      "  train_runtime            = 0:01:49.29\n",
      "  train_samples_per_second =       7.32\n",
      "  train_steps_per_second   =      0.476\n"
     ]
    }
   ],
   "source": [
    "train_results = trainer.train()\n",
    "trainer.save_model()\n",
    "trainer.log_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "72320513-3d1e-4470-9dc9-0a5558e280ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='25' max='25' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [25/25 00:10]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =        4.0\n",
      "  eval_accuracy           =        0.9\n",
      "  eval_loss               =      4.096\n",
      "  eval_runtime            = 0:00:11.32\n",
      "  eval_samples_per_second =     17.654\n",
      "  eval_steps_per_second   =      2.207\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.evaluate(prepared_test_ds)\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f7ac12-efe4-48a7-a76b-e318c43b4c67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
