{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "c5f86c9a-fbf1-4bb1-80b2-9d9018f048d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ViT dataset pre-processing and training notebook\n",
    "\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "from PIL import ImageDraw, ImageFont, Image\n",
    "from transformers import ViTImageProcessor, ViTForImageClassification, TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "from evaluate import load\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import os\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "041d14e6-7fa6-44da-8307-d57730be6e7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nUtility function\\ndef populate_random_datasets():\\n    dataset = []\\n    dataset.append(random.sample(family_sedans, 2))\\n    dataset.append(random.sample(suv, 2))\\n    dataset.append(random.sample(coupe, 2))\\n    dataset.append(random.sample(convertible, 2))\\n    dataset.append(random.sample(misfits, 4))\\n    dataset = [item for sublist in dataset for item in sublist]\\n    return dataset\\n    \\nrandom_1 = populate_random_datasets()\\nrandom_2 = populate_random_datasets()\\n'"
      ]
     },
     "execution_count": 248,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "family_sedans = [2, 3, 4, 5, 20, 23, 24, 26, 29, 35, 44, 47, 49, 51, 61, 63, 66, 67, 73, 79, 93, 97, 144, 152, 96, \n",
    "                 105, 115, 117, 129, 134, 135, 136, 137, 138, 140, 156, 162, 164, 165, 167, 173, 176, 177, 181, 182, \n",
    "                 184, 185, 187, 188, 194]\n",
    "suv = [32, 33, 37, 48, 50, 52, 58, 62, 68, 76, 89, 94, 95, 109, 110, 118, 120, 121, 131, 132, 133, 142, 143, 145, \n",
    "       146, 147, 148, 149, 154, 155, 159, 186, 189, 195]\n",
    "coupe = [9, 11, 15, 42, 43, 13, 14, 22, 25, 28, 34, 72, 77, 80, 81, 100, 101, 102, 103, 104, 107, 112, 123, 128, \n",
    "         141, 150, 151, 153, 157, 158, 160, 161, 163, 171, 172, 175, 179, 180, 196]\n",
    "convertible = [8, 10, 39, 12, 21, 27, 31, 36, 38, 55, 59]\n",
    "misfits = [1, 18, 30, 40, 41, 45, 53, 46, 54, 56, 57, 60, 64, 69, 65, 70, 71, 7, 19, 6, 16, 17, 74, 75, 78, 82, 83, 84, \n",
    "           85, 86, 87, 88, 90, 91, 92, 98, 99, 106, 108, 111, 113, 114, 116, 119, 122, 124, 125, 126, 127, 130, 139, 166, \n",
    "           168, 169, 170, 174, 178, 183, 190, 191, 192, 193]\n",
    "random_1 = [16, 40, 1, 54, 46, 7, 41, 60, 18, 57, 5, 2, 29, 26, 73, 4, 47, 3, 44, 66, 59, 10, 8, 21, 55, 31, 150, 11, 151, 100, 80]\n",
    "random_2 = [1, 10, 15, 25, 45, 75]\n",
    "\n",
    "\"\"\"\n",
    "Utility function\n",
    "def populate_random_datasets():\n",
    "    dataset = []\n",
    "    dataset.append(random.sample(family_sedans, 2))\n",
    "    dataset.append(random.sample(suv, 2))\n",
    "    dataset.append(random.sample(coupe, 2))\n",
    "    dataset.append(random.sample(convertible, 2))\n",
    "    dataset.append(random.sample(misfits, 4))\n",
    "    dataset = [item for sublist in dataset for item in sublist]\n",
    "    return dataset\n",
    "    \n",
    "random_1 = populate_random_datasets()\n",
    "random_2 = populate_random_datasets()\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "90b73952-affa-4fc3-9260-95493b221d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "seam_carved_datasets = True\n",
    "\n",
    "#metadata_filepath = \"StanfordCars/stanford_cars_with_class_names.xlsx\"\n",
    "train_metadata_df = pd.read_excel(\n",
    "    \"StanfordCars/stanford_cars_with_class_names.xlsx\", sheet_name='train'\n",
    ")\n",
    "test_metadata_df = pd.read_excel(\n",
    "    \"StanfordCars/stanford_cars_with_class_names_test.xlsx\", sheet_name='test'\n",
    ")\n",
    "\n",
    "# Update these folders \n",
    "train_rg_folder = \"StanfordCars/cars_train/cars_train\"\n",
    "#train_sc_folder = \"StanfordCars/Train\"\n",
    "train_sc_folder = \"StanfordCars/cars_train_sc\"\n",
    "\n",
    "test_rg_folder = \"StanfordCars/cars_test/cars_test\"\n",
    "#test_sc_folder = \"StanfordCars/Test\"\n",
    "test_sc_folder = \"StanfordCars/cars_test_sc\"\n",
    "\n",
    "if seam_carved_datasets:\n",
    "    train_folder = train_sc_folder\n",
    "    test_folder = test_sc_folder\n",
    "else: \n",
    "    train_folder = train_rg_folder\n",
    "    test_folder = test_rg_folder\n",
    "\n",
    "folders = {\n",
    "    \"train\": (train_folder, train_metadata_df),\n",
    "    \"test\": (test_folder, test_metadata_df),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "6c0d335b-368e-48ea-94bd-5d6424899474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label ID 0: 86 images\n",
      "Label ID 1: 66 images\n",
      "Label ID 2: 88 images\n",
      "Label ID 3: 88 images\n",
      "Label ID 4: 65 images\n",
      "Label ID 5: 79 images\n"
     ]
    }
   ],
   "source": [
    "# Load dataset\n",
    "image_label_pairs =[]\n",
    "class_id_to_label_id = {}\n",
    "class_name_to_label_id = {}\n",
    "label_id_to_class_name = {}\n",
    "\n",
    "# Reset labels from 0\n",
    "next_label_id = 0\n",
    "\n",
    "for _k, (folder, df) in folders.items():\n",
    "    image_files = [f for f in os.listdir(folder) if f.lower()\n",
    "                     .endswith(('png', 'jpg', 'jpeg', 'bmp', 'tiff'))]\n",
    "    \n",
    "    for image_file in image_files:\n",
    "        image_path = os.path.join(folder, image_file)\n",
    "    \n",
    "        try:\n",
    "            with Image.open(image_path) as img:\n",
    "    \n",
    "                # There are a few Grayscale images (less than 0.1%) in the dataset \n",
    "                # that we do not consider since the downstream ViT expects three \n",
    "                # input channels (RGB) for each image\n",
    "                if img.mode != \"RGB\":\n",
    "                    continue\n",
    "    \n",
    "                metadata_filename = image_file.replace(\"_sc\", \"\") if seam_carved_datasets else image_file\n",
    "                metadata = df.loc[df['image'] == metadata_filename]\n",
    "                \n",
    "                if not metadata.empty:\n",
    "                    class_name = metadata['ture_class_name'].values[0]\n",
    "                    class_id = metadata['class'].values[0]\n",
    "                    # replace this with subset, comment out if testing on full dataset\n",
    "                    if class_id not in random_2: \n",
    "                       continue \n",
    "    \n",
    "                    if class_id not in class_id_to_label_id:\n",
    "                        class_id_to_label_id[class_id] = next_label_id\n",
    "                        class_name_to_label_id[class_name] = next_label_id\n",
    "                        label_id_to_class_name[next_label_id] = class_name\n",
    "                        next_label_id += 1\n",
    "                    \n",
    "                    label_id = class_id_to_label_id[class_id]\n",
    "                    image_label_pairs.append((img, label_id))\n",
    "                else:\n",
    "                    print(f\"Could not fine metadata for {image_file}.\") \n",
    "        except ValueError as ve:\n",
    "            print(f\"ValueError encountered with image: {image_file}, skipping it.\\n\")\n",
    "\n",
    "\n",
    "label_counts = Counter(label_id for _, label_id in image_label_pairs)\n",
    "for label_id, count in label_counts.items():\n",
    "    print(f\"Label ID {label_id}: {count} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "9285e001-35f1-44a3-aea7-8f8daced9395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training pairs: 375\n",
      "Total testing pairs: 97\n",
      "\n",
      "Training set distribution:\n",
      "Label ID 0: 68 images\n",
      "Label ID 1: 52 images\n",
      "Label ID 2: 70 images\n",
      "Label ID 3: 70 images\n",
      "Label ID 4: 52 images\n",
      "Label ID 5: 63 images\n",
      "\n",
      "Testing set distribution:\n",
      "Label ID 0: 18 images\n",
      "Label ID 1: 14 images\n",
      "Label ID 2: 18 images\n",
      "Label ID 3: 18 images\n",
      "Label ID 4: 13 images\n",
      "Label ID 5: 16 images\n"
     ]
    }
   ],
   "source": [
    "# Generate training and test datasets\n",
    "split_ratio = 0.8\n",
    "\n",
    "train_pairs = []\n",
    "test_pairs = []\n",
    "\n",
    "label_to_pairs = defaultdict(list)\n",
    "for image, label_id in image_label_pairs:\n",
    "    label_to_pairs[label_id].append((image, label_id))\n",
    "\n",
    "for label_id, pairs in label_to_pairs.items():\n",
    "    #random.shuffle(pairs) \n",
    "    sorted_pairs = sorted(\n",
    "        pairs,\n",
    "        key=lambda x: x[0].filename.split(\"/\")[-1]  # Extract and sort by the filename\n",
    "    )\n",
    "    split_index = int(len(pairs) * split_ratio)  \n",
    "    train_pairs.extend(pairs[:split_index])    \n",
    "    test_pairs.extend(pairs[split_index:])      \n",
    "\n",
    "print(f\"Total training pairs: {len(train_pairs)}\")\n",
    "print(f\"Total testing pairs: {len(test_pairs)}\\n\")\n",
    "\n",
    "train_counts = Counter(label_id for _, label_id in train_pairs)\n",
    "test_counts = Counter(label_id for _, label_id in test_pairs)\n",
    "\n",
    "print(\"Training set distribution:\")\n",
    "for label_id, count in train_counts.items():\n",
    "    print(f\"Label ID {label_id}: {count} images\")\n",
    "\n",
    "print(\"\\nTesting set distribution:\")\n",
    "for label_id, count in test_counts.items():\n",
    "    print(f\"Label ID {label_id}: {count} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "b8330be9-8e8a-4fa3-bb9e-dadf1e3a8705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format datasets so that they can be consumed by HuggingFace APIs\n",
    "\n",
    "train_images = [image for image, _ in train_pairs]\n",
    "train_labels = [label for _, label in train_pairs]\n",
    "\n",
    "train_data_dict = {\n",
    "    \"image\": train_images,\n",
    "    \"labels\": train_labels,\n",
    "}\n",
    "train_dataset = Dataset.from_dict(train_data_dict) # Loading into a HuggingFace Dataset object\n",
    "\n",
    "test_images = [image for image, _ in test_pairs]\n",
    "test_labels = [label for _, label in test_pairs]\n",
    "\n",
    "test_data_dict = {\n",
    "    \"image\": test_images,\n",
    "    \"labels\": test_labels,\n",
    "}\n",
    "test_dataset = Dataset.from_dict(test_data_dict) # Loading into a HuggingFace Dataset object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "b8f54c90-6c43-4181-94af-77a275ef3601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTImageProcessor {\n",
       "  \"do_normalize\": true,\n",
       "  \"do_rescale\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"image_mean\": [\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5\n",
       "  ],\n",
       "  \"image_processor_type\": \"ViTImageProcessor\",\n",
       "  \"image_std\": [\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5\n",
       "  ],\n",
       "  \"resample\": 2,\n",
       "  \"rescale_factor\": 0.00392156862745098,\n",
       "  \"size\": {\n",
       "    \"height\": 224,\n",
       "    \"width\": 224\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load Image processor\n",
    "model_name_or_path = 'google/vit-base-patch16-224-in21k'\n",
    "processor = ViTImageProcessor.from_pretrained(model_name_or_path)\n",
    "processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "3896633c-0b8b-45b3-83cb-a27a7a10bf81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pixel_values': tensor([[[0.6549, 0.6627, 0.7255,  ..., 0.9686, 0.9922, 0.9922],\n",
       "          [0.6941, 0.7020, 0.7569,  ..., 0.9843, 1.0000, 0.9843],\n",
       "          [0.7098, 0.7176, 0.7647,  ..., 0.9922, 1.0000, 0.9765],\n",
       "          ...,\n",
       "          [0.6471, 0.6941, 0.6941,  ..., 0.8510, 0.8902, 0.8980],\n",
       "          [0.6314, 0.7176, 0.6784,  ..., 0.8824, 0.9216, 0.9373],\n",
       "          [0.6392, 0.6706, 0.5843,  ..., 0.8667, 0.9059, 0.9216]],\n",
       " \n",
       "         [[0.4745, 0.4824, 0.5137,  ..., 0.9765, 0.9922, 0.9922],\n",
       "          [0.5137, 0.5216, 0.5451,  ..., 0.9922, 1.0000, 0.9843],\n",
       "          [0.5294, 0.5373, 0.5529,  ..., 1.0000, 1.0000, 0.9765],\n",
       "          ...,\n",
       "          [0.4980, 0.5608, 0.5451,  ..., 0.8510, 0.8118, 0.8118],\n",
       "          [0.4667, 0.5765, 0.5137,  ..., 0.8824, 0.8510, 0.8275],\n",
       "          [0.4745, 0.5059, 0.4196,  ..., 0.8667, 0.8353, 0.8196]],\n",
       " \n",
       "         [[0.2314, 0.2314, 0.2784,  ..., 1.0000, 1.0000, 1.0000],\n",
       "          [0.2706, 0.2706, 0.3098,  ..., 1.0000, 1.0000, 0.9843],\n",
       "          [0.2863, 0.2863, 0.3176,  ..., 1.0000, 1.0000, 0.9608],\n",
       "          ...,\n",
       "          [0.3647, 0.4196, 0.4118,  ..., 0.7725, 0.7961, 0.7961],\n",
       "          [0.3176, 0.4196, 0.3647,  ..., 0.8196, 0.8118, 0.8039],\n",
       "          [0.3255, 0.3569, 0.2706,  ..., 0.8039, 0.7804, 0.7725]]]),\n",
       " 'labels': 0}"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transform images so ViT can understand them\n",
    "def transform(example_batch):\n",
    "    # Take a list of PIL images and turn them to pixel values\n",
    "    inputs = processor([x for x in example_batch['image']], return_tensors='pt')\n",
    "\n",
    "    inputs['labels'] = example_batch['labels']\n",
    "    return inputs\n",
    "\n",
    "prepared_train_ds = train_dataset.with_transform(transform)\n",
    "prepared_test_ds = test_dataset.with_transform(transform)\n",
    "prepared_train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "64191a9a-0520-4d5c-9aa0-1eb163c8f211",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Setup model\n",
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
    "        'labels': torch.tensor([x['labels'] for x in batch])\n",
    "    }\n",
    "\n",
    "metric = load(\"accuracy\")\n",
    "def compute_metrics(p):\n",
    "    return metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids)\n",
    "\n",
    "labels = list(class_id_to_label_id.keys())\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    num_labels=len(labels),\n",
    "    id2label=label_id_to_class_name,\n",
    "    label2id=class_name_to_label_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "4dd7ea17-3866-4f56-83e1-40fa142f18a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/training_args.py:1583: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of ðŸ¤— Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n",
      "/var/folders/xx/6trqhb9n4yq6254vft620z0r0000gn/T/ipykernel_40449/3125884726.py:19: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# Setup training parameters and Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./vit-model\",\n",
    "    per_device_train_batch_size=16,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    num_train_epochs=20,\n",
    "    fp16=False,\n",
    "    no_cuda=True, # If you have access to a GPU, set this to False to speedup training\n",
    "    save_steps=100,\n",
    "    eval_steps=100,\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-4,\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    "    report_to='tensorboard',\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=prepared_train_ds,\n",
    "    eval_dataset=prepared_test_ds,\n",
    "    tokenizer=processor,\n",
    "    data_collator=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "34c2c1cb-7245-4405-aa23-e80edeb55337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='480' max='480' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [480/480 15:48, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.050000</td>\n",
       "      <td>0.195372</td>\n",
       "      <td>0.938144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.020200</td>\n",
       "      <td>0.137147</td>\n",
       "      <td>0.969072</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.013200</td>\n",
       "      <td>0.131128</td>\n",
       "      <td>0.979381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.010600</td>\n",
       "      <td>0.128859</td>\n",
       "      <td>0.979381</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =        20.0\n",
      "  total_flos               = 541294699GF\n",
      "  train_loss               =      0.1017\n",
      "  train_runtime            =  0:15:51.15\n",
      "  train_samples_per_second =       7.885\n",
      "  train_steps_per_second   =       0.505\n"
     ]
    }
   ],
   "source": [
    "# Train \n",
    "train_results = trainer.train()\n",
    "trainer.save_model()\n",
    "trainer.log_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "b7994bc5-2329-42ac-9efb-3d2aef260f40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:03]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =       20.0\n",
      "  eval_accuracy           =     0.9794\n",
      "  eval_loss               =     0.1289\n",
      "  eval_runtime            = 0:00:04.02\n",
      "  eval_samples_per_second =     24.084\n",
      "  eval_steps_per_second   =      3.228\n"
     ]
    }
   ],
   "source": [
    "# Eval\n",
    "metrics = trainer.evaluate(prepared_test_ds)\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4ae449-8920-492c-b8a1-e7a908113106",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
