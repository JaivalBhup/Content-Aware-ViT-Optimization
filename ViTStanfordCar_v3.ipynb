{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c5f86c9a-fbf1-4bb1-80b2-9d9018f048d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "import random\n",
    "from PIL import ImageDraw, ImageFont, Image\n",
    "from transformers import ViTImageProcessor, ViTForImageClassification, TrainingArguments, Trainer\n",
    "import numpy as np\n",
    "from evaluate import load\n",
    "import pandas as pd\n",
    "from datasets import Dataset\n",
    "import os\n",
    "from collections import Counter, defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "90b73952-affa-4fc3-9260-95493b221d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "seam_carved_datasets = False\n",
    "\n",
    "#metadata_filepath = \"StanfordCars/stanford_cars_with_class_names.xlsx\"\n",
    "train_metadata_df = pd.read_excel(\n",
    "    \"StanfordCars/stanford_cars_with_class_names.xlsx\", sheet_name='train'\n",
    ")\n",
    "test_metadata_df = pd.read_excel(\n",
    "    \"StanfordCars/stanford_cars_with_class_names_test.xlsx\", sheet_name='test'\n",
    ")\n",
    "\n",
    "train_rg_folder = \"StanfordCars/cars_train/cars_train\"\n",
    "train_sc_folder = \"StanfordCars/Train\"\n",
    "\n",
    "test_rg_folder = \"StanfordCars/cars_test/cars_test\"\n",
    "test_sc_folder = \"StanfordCars/Test\"\n",
    "\n",
    "if seam_carved_datasets:\n",
    "    train_folder = train_sc_folder\n",
    "    test_folder = test_sc_folder\n",
    "else: \n",
    "    train_folder = train_rg_folder\n",
    "    test_folder = test_rg_folder\n",
    "\n",
    "folders = {\n",
    "    \"train\": (train_folder, train_metadata_df),\n",
    "    \"test\": (test_folder, test_metadata_df),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "6c0d335b-368e-48ea-94bd-5d6424899474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label ID 0: 79 images\n",
      "Label ID 1: 86 images\n",
      "Label ID 2: 88 images\n",
      "Label ID 3: 66 images\n",
      "Label ID 4: 65 images\n",
      "Label ID 5: 88 images\n"
     ]
    }
   ],
   "source": [
    "image_label_pairs =[]\n",
    "class_id_to_label_id = {}\n",
    "class_name_to_label_id = {}\n",
    "label_id_to_class_name = {}\n",
    "next_label_id = 0\n",
    "\n",
    "for _k, (folder, df) in folders.items():\n",
    "    image_files = [f for f in os.listdir(folder) if f.lower()\n",
    "                     .endswith(('png', 'jpg', 'jpeg', 'bmp', 'tiff'))]\n",
    "    \n",
    "    for image_file in image_files:\n",
    "        image_path = os.path.join(folder, image_file)\n",
    "    \n",
    "        try:\n",
    "            with Image.open(image_path) as img:\n",
    "    \n",
    "                # There are a few Grayscale images (less than 0.1%) in the dataset \n",
    "                # that we do not consider since the downstream ViT expects three \n",
    "                # input channels (RGB) for each image\n",
    "                if img.mode != \"RGB\":\n",
    "                    continue\n",
    "    \n",
    "                metadata_filename = image_file.replace(\"_sc\", \"\") if seam_carved_datasets else image_file\n",
    "                metadata = df.loc[df['image'] == metadata_filename]\n",
    "                \n",
    "                if not metadata.empty:\n",
    "                    class_name = metadata['ture_class_name'].values[0]\n",
    "                    class_id = metadata['class'].values[0]\n",
    "                    if class_id not in [1,10,15,25,45,75]:\n",
    "                       continue \n",
    "    \n",
    "                    if class_id not in class_id_to_label_id:\n",
    "                        class_id_to_label_id[class_id] = next_label_id\n",
    "                        class_name_to_label_id[class_name] = next_label_id\n",
    "                        label_id_to_class_name[next_label_id] = class_name\n",
    "                        next_label_id += 1\n",
    "                    \n",
    "                    label_id = class_id_to_label_id[class_id]\n",
    "                    image_label_pairs.append((img, label_id))\n",
    "                else:\n",
    "                    print(f\"Could not fine metadata for {image_file}.\") \n",
    "        except ValueError as ve:\n",
    "            print(f\"ValueError encountered with image: {image_file}, skipping it.\\n\")\n",
    "\n",
    "\n",
    "label_counts = Counter(label_id for _, label_id in image_label_pairs)\n",
    "for label_id, count in label_counts.items():\n",
    "    print(f\"Label ID {label_id}: {count} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9285e001-35f1-44a3-aea7-8f8daced9395",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training pairs: 375\n",
      "Total testing pairs: 97\n",
      "\n",
      "Training set distribution:\n",
      "Label ID 0: 63 images\n",
      "Label ID 1: 68 images\n",
      "Label ID 2: 70 images\n",
      "Label ID 3: 52 images\n",
      "Label ID 4: 52 images\n",
      "Label ID 5: 70 images\n",
      "\n",
      "Testing set distribution:\n",
      "Label ID 0: 16 images\n",
      "Label ID 1: 18 images\n",
      "Label ID 2: 18 images\n",
      "Label ID 3: 14 images\n",
      "Label ID 4: 13 images\n",
      "Label ID 5: 18 images\n"
     ]
    }
   ],
   "source": [
    "# Generate training and test datasets\n",
    "split_ratio = 0.8\n",
    "\n",
    "train_pairs = []\n",
    "test_pairs = []\n",
    "\n",
    "label_to_pairs = defaultdict(list)\n",
    "for image, label_id in image_label_pairs:\n",
    "    label_to_pairs[label_id].append((image, label_id))\n",
    "\n",
    "for label_id, pairs in label_to_pairs.items():\n",
    "    #random.shuffle(pairs) \n",
    "    sorted_pairs = sorted(\n",
    "        pairs,\n",
    "        key=lambda x: x[0].filename.split(\"/\")[-1]  # Extract and sort by the filename\n",
    "    )\n",
    "    split_index = int(len(pairs) * split_ratio)  \n",
    "    train_pairs.extend(pairs[:split_index])    \n",
    "    test_pairs.extend(pairs[split_index:])      \n",
    "\n",
    "print(f\"Total training pairs: {len(train_pairs)}\")\n",
    "print(f\"Total testing pairs: {len(test_pairs)}\\n\")\n",
    "\n",
    "train_counts = Counter(label_id for _, label_id in train_pairs)\n",
    "test_counts = Counter(label_id for _, label_id in test_pairs)\n",
    "\n",
    "print(\"Training set distribution:\")\n",
    "for label_id, count in train_counts.items():\n",
    "    print(f\"Label ID {label_id}: {count} images\")\n",
    "\n",
    "print(\"\\nTesting set distribution:\")\n",
    "for label_id, count in test_counts.items():\n",
    "    print(f\"Label ID {label_id}: {count} images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "b8330be9-8e8a-4fa3-bb9e-dadf1e3a8705",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate datasets that can be consumed by HuggingFace APIs\n",
    "\n",
    "train_images = [image for image, _ in train_pairs]\n",
    "train_labels = [label for _, label in train_pairs]\n",
    "\n",
    "train_data_dict = {\n",
    "    \"image\": train_images,\n",
    "    \"labels\": train_labels,\n",
    "}\n",
    "train_dataset = Dataset.from_dict(train_data_dict) # Loading into a HuggingFace Dataset object\n",
    "\n",
    "test_images = [image for image, _ in test_pairs]\n",
    "test_labels = [label for _, label in test_pairs]\n",
    "\n",
    "test_data_dict = {\n",
    "    \"image\": test_images,\n",
    "    \"labels\": test_labels,\n",
    "}\n",
    "test_dataset = Dataset.from_dict(test_data_dict) # Loading into a HuggingFace Dataset object\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "b8f54c90-6c43-4181-94af-77a275ef3601",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ViTImageProcessor {\n",
       "  \"do_normalize\": true,\n",
       "  \"do_rescale\": true,\n",
       "  \"do_resize\": true,\n",
       "  \"image_mean\": [\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5\n",
       "  ],\n",
       "  \"image_processor_type\": \"ViTImageProcessor\",\n",
       "  \"image_std\": [\n",
       "    0.5,\n",
       "    0.5,\n",
       "    0.5\n",
       "  ],\n",
       "  \"resample\": 2,\n",
       "  \"rescale_factor\": 0.00392156862745098,\n",
       "  \"size\": {\n",
       "    \"height\": 224,\n",
       "    \"width\": 224\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_name_or_path = 'google/vit-base-patch16-224-in21k'\n",
    "processor = ViTImageProcessor.from_pretrained(model_name_or_path)\n",
    "processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3896633c-0b8b-45b3-83cb-a27a7a10bf81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'pixel_values': tensor([[[-0.2627, -0.2078, -0.1922,  ...,  0.8824,  0.8824,  0.8824],\n",
       "          [-0.2471, -0.1608, -0.1451,  ...,  0.8824,  0.8824,  0.8745],\n",
       "          [-0.2471, -0.1451, -0.1137,  ...,  0.8824,  0.8824,  0.8745],\n",
       "          ...,\n",
       "          [ 0.5373,  0.5451,  0.5608,  ...,  0.7961,  0.7961,  0.7961],\n",
       "          [ 0.5216,  0.5294,  0.5373,  ...,  0.7961,  0.7961,  0.7961],\n",
       "          [ 0.5216,  0.5294,  0.5294,  ...,  0.7882,  0.7882,  0.7882]],\n",
       " \n",
       "         [[-0.2314, -0.1765, -0.1686,  ...,  0.8902,  0.8902,  0.8902],\n",
       "          [-0.2157, -0.1294, -0.1216,  ...,  0.8902,  0.8902,  0.8824],\n",
       "          [-0.2000, -0.1059, -0.0745,  ...,  0.8902,  0.8902,  0.8824],\n",
       "          ...,\n",
       "          [ 0.5686,  0.5765,  0.5922,  ...,  0.8039,  0.8039,  0.8039],\n",
       "          [ 0.5451,  0.5608,  0.5608,  ...,  0.8039,  0.8039,  0.8039],\n",
       "          [ 0.5451,  0.5529,  0.5529,  ...,  0.7961,  0.7961,  0.7961]],\n",
       " \n",
       "         [[ 0.0510,  0.1451,  0.1608,  ...,  0.9294,  0.9294,  0.9294],\n",
       "          [ 0.0431,  0.1765,  0.1922,  ...,  0.9294,  0.9294,  0.9216],\n",
       "          [ 0.0275,  0.1765,  0.2235,  ...,  0.9294,  0.9294,  0.9216],\n",
       "          ...,\n",
       "          [ 0.6549,  0.6627,  0.6784,  ...,  0.8431,  0.8431,  0.8431],\n",
       "          [ 0.6235,  0.6314,  0.6314,  ...,  0.8431,  0.8431,  0.8431],\n",
       "          [ 0.6000,  0.6078,  0.6078,  ...,  0.8353,  0.8353,  0.8353]]]),\n",
       " 'labels': 0}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def transform(example_batch):\n",
    "    # Take a list of PIL images and turn them to pixel values\n",
    "    inputs = processor([x for x in example_batch['image']], return_tensors='pt')\n",
    "\n",
    "    inputs['labels'] = example_batch['labels']\n",
    "    return inputs\n",
    "\n",
    "prepared_train_ds = train_dataset.with_transform(transform)\n",
    "prepared_test_ds = test_dataset.with_transform(transform)\n",
    "prepared_train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "64191a9a-0520-4d5c-9aa0-1eb163c8f211",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224-in21k and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "def collate_fn(batch):\n",
    "    return {\n",
    "        'pixel_values': torch.stack([x['pixel_values'] for x in batch]),\n",
    "        'labels': torch.tensor([x['labels'] for x in batch])\n",
    "    }\n",
    "\n",
    "metric = load(\"accuracy\")\n",
    "def compute_metrics(p):\n",
    "    return metric.compute(predictions=np.argmax(p.predictions, axis=1), references=p.label_ids)\n",
    "\n",
    "labels = list(class_id_to_label_id.keys())\n",
    "model = ViTForImageClassification.from_pretrained(\n",
    "    model_name_or_path,\n",
    "    num_labels=len(labels),\n",
    "    id2label=label_id_to_class_name,\n",
    "    label2id=class_name_to_label_id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4dd7ea17-3866-4f56-83e1-40fa142f18a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/training_args.py:1568: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/transformers/training_args.py:1583: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of ðŸ¤— Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n",
      "/var/folders/xx/6trqhb9n4yq6254vft620z0r0000gn/T/ipykernel_40449/3125884726.py:19: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./vit-model\",\n",
    "    per_device_train_batch_size=16,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    num_train_epochs=20,\n",
    "    fp16=False,\n",
    "    no_cuda=True,\n",
    "    save_steps=100,\n",
    "    eval_steps=100,\n",
    "    logging_steps=10,\n",
    "    learning_rate=2e-4,\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    "    push_to_hub=False,\n",
    "    report_to='tensorboard',\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    compute_metrics=compute_metrics,\n",
    "    train_dataset=prepared_train_ds,\n",
    "    eval_dataset=prepared_test_ds,\n",
    "    tokenizer=processor,\n",
    "    data_collator=collate_fn,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "34c2c1cb-7245-4405-aa23-e80edeb55337",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='480' max='480' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [480/480 17:18, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.044700</td>\n",
       "      <td>0.174697</td>\n",
       "      <td>0.958763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.019800</td>\n",
       "      <td>0.153308</td>\n",
       "      <td>0.958763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.013300</td>\n",
       "      <td>0.151796</td>\n",
       "      <td>0.958763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>400</td>\n",
       "      <td>0.010500</td>\n",
       "      <td>0.153833</td>\n",
       "      <td>0.958763</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =        20.0\n",
      "  total_flos               = 541294699GF\n",
      "  train_loss               =      0.0871\n",
      "  train_runtime            =  0:17:20.90\n",
      "  train_samples_per_second =       7.205\n",
      "  train_steps_per_second   =       0.461\n"
     ]
    }
   ],
   "source": [
    "train_results = trainer.train()\n",
    "trainer.save_model()\n",
    "trainer.log_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_metrics(\"train\", train_results.metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b7994bc5-2329-42ac-9efb-3d2aef260f40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='13' max='13' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [13/13 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =       20.0\n",
      "  eval_accuracy           =     0.9588\n",
      "  eval_loss               =     0.1518\n",
      "  eval_runtime            = 0:00:05.17\n",
      "  eval_samples_per_second =     18.759\n",
      "  eval_steps_per_second   =      2.514\n"
     ]
    }
   ],
   "source": [
    "metrics = trainer.evaluate(prepared_test_ds)\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
